{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cascade_MNISTSaveActivation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNm6ZktwEGNZEILeYU3eZ+V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcash8/Info-Theory-Project/blob/main/CL%20Learning%20Experiments/Cascade_MNISTSaveActivation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1X3aZhzdLTA",
        "outputId": "369207d8-0380-41a7-cc93-21a323424ea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/Cascade/Cascade_calculate_loaded_data_information1.py /content/\n",
        "!cp drive/MyDrive/Cascade/kde.py /content/"
      ],
      "metadata": {
        "id": "x_ck6y6RgJVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from keras.layers.core import Dense\n",
        "import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "import tqdm\n",
        "import time\n",
        "import keras.backend as K\n",
        "from time import sleep\n",
        "import os\n",
        "import pickle as cPickle\n",
        "import matplotlib.pyplot as plt\n",
        "from Cascade_calculate_loaded_data_information1 import calculate_layer_mutual_information,extract_probs,calculate_mutual_information,calculate_lower_mutual_information\n",
        "import h5py\n",
        "from keras.datasets import mnist"
      ],
      "metadata": {
        "id": "rgjAAWIUfVQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy==1.1.0"
      ],
      "metadata": {
        "id": "ZHT8xXDIjT1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#================================================================================================\n",
        "def load_generated_data(data_name):\n",
        "    \"load generated data\"\n",
        "    d=h5py.File(data_name)\n",
        "    data=dict()\n",
        "    data['F']=[]\n",
        "    data['y']=[]\n",
        "    data['F']=np.array(d['F']).T\n",
        "    data['y']=np.array(d['y'])\n",
        "    return data\n",
        "def shuffle_in_unison_inplace(a, b):\n",
        "    \"\"\"Shuffle the arrays randomly\"\"\"\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "def data_label_shuffle(data_sets_org, percent_of_train, min_test_data=80, shuffle_data=True):\n",
        "    \"\"\"Divided the data to train and test and shuffle it\"\"\"\n",
        "    perc = lambda i, t: np.rint((i * t) / 100).astype(np.int32)\n",
        "    C = type('type_C', (object,), {})\n",
        "    data_sets = C()\n",
        "    stop_train_index = perc(percent_of_train[0], data_sets_org['F'].shape[0])\n",
        "    start_test_index = stop_train_index\n",
        "    if percent_of_train > min_test_data:\n",
        "        start_test_index = perc(min_test_data, data_sets_org['F'].shape[0])\n",
        "    data_sets.train = C()\n",
        "    data_sets.test = C()\n",
        "    if shuffle_data:\n",
        "        shuffled_data, shuffled_labels = shuffle_in_unison_inplace(data_sets_org['F'], data_sets_org['y'])\n",
        "    else:\n",
        "        shuffled_data, shuffled_labels = data_sets_org.data, data_sets_org.labels\n",
        "    data_sets.train.data = shuffled_data[:stop_train_index, :]\n",
        "    data_sets.train.labels = shuffled_labels[:stop_train_index, :]\n",
        "    data_sets.test.data = shuffled_data[start_test_index:, :]\n",
        "    data_sets.test.labels = shuffled_labels[start_test_index:, :]\n",
        "    return data_sets\n",
        "def store_file(stringOfHistory,his):\n",
        "    \"Store the history of accuarcy\"\n",
        "    with open(stringOfHistory,'wb') as fp:\n",
        "        return cPickle.dump(his,fp)\n",
        "def load_DATA(name):\n",
        "    \"Load history\"\n",
        "    with open(name, 'rb') as f:\n",
        "        data=cPickle.load(f)\n",
        "        return data\n",
        "#========================================================================================================================================================================================\n",
        "class OutputObserver(keras.callbacks.Callback):\n",
        "    'callback to observe the output of the network'\n",
        "    def __init__(self, data_set,layernum,DATA,Label,bins,logs={}):\n",
        "        self.out_log = dict()  # Save the output of each layer in each epoch\n",
        "        self.out_log['eachlayer']=[]\n",
        "        self.out_log['each_final_layer']=[]\n",
        "        self.data_set = data_set\n",
        "        self.layer_name='Layer'+str(layernum)\n",
        "        self.DATA=DATA \n",
        "        self.Label=Label\n",
        "        self.bins=bins\n",
        "        self.local_IXT_cas_layer,self.local_ITY_cas_layer,self.finallocal_IXT_cas_layer,self.finallocal_ITY_cas_layer=[],[],[],[] # Save the information calculated by Binning method\n",
        "        self.KDElocal_IXT_cas_layer,self.KDElocal_ITY_cas_layer,self.KDEfinallocal_IXT_cas_layer,self.KDEfinallocal_ITY_cas_layer=[],[],[],[] # Save the information calculated by PWD based upper bound method\n",
        "        self.kdislocal_IXT_cas_layer,self.kdislocal_ITY_cas_layer,self.kdisfinallocal_IXT_cas_layer,self.kdisfinallocal_ITY_cas_layer=[],[],[],[]  # Save the information calculated by PWD based lower bound method\n",
        "        self.pys1,self.unique_inverse_x,self. unique_inverse_y,self. pxs = extract_probs(self.Label,self.data_set) \n",
        "        self.PXs, self.PYs = np.asarray(self.pxs).T, np.asarray(self.pys1).T  # The distributation of Input and Label \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \"Calculate and save the information for each epoch\"\n",
        "        intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(self.layer_name).output) \n",
        "        layeroutput_cas=intermediate_layer_model.predict(self.data_set)                                           # Get the output of layer with name self.layer_name\n",
        "        intermediate_predict_layer_model = Model(inputs=model.input,outputs=model.get_layer('Prediction_Layer').output) \n",
        "        finallayeroutput_cas=intermediate_predict_layer_model.predict(self.data_set)                              # Get the output of the classifier which is following the layer with name self.layer_name\n",
        "        self.out_log['eachlayer'].append(layeroutput_cas)\n",
        "        self.out_log['each_final_layer'].append(finallayeroutput_cas)\n",
        "#        ==============================================================\n",
        "        IX_CAS2,IY_CAS2=calculate_lower_mutual_information(layeroutput_cas[0:len(self.DATA)],self.PXs, self.PYs,self.unique_inverse_x,self.unique_inverse_y) # Calculate information for cascaded layer by PWD based lower bound method\n",
        "        finalIX_CAS2,finalIY_CAS2=calculate_lower_mutual_information(finallayeroutput_cas[0:len(self.DATA)],self.PXs, self.PYs,self.unique_inverse_x,self.unique_inverse_y) # Calculate information for the classifier following the cascaded layer\n",
        "        print('KdisI(X;T) is:'+str(IX_CAS2),'KdisI(Y;T) is:'+str(IY_CAS2))\n",
        "        print('KdisI(X;T) is:'+str(finalIX_CAS2),'KdisI(Y;T) is:'+str(finalIY_CAS2))\n",
        "        self.kdislocal_IXT_cas_layer.append(IX_CAS2)\n",
        "        self.kdislocal_ITY_cas_layer.append(IY_CAS2)\n",
        "        self.kdisfinallocal_IXT_cas_layer.append(finalIX_CAS2)\n",
        "        self.kdisfinallocal_ITY_cas_layer.append(finalIY_CAS2)\n",
        "#        ==============================================================\n",
        "        IX_CAS,IY_CAS=calculate_mutual_information(layeroutput_cas[0:len(self.DATA)],self.PXs, self.PYs,self.unique_inverse_x,self.unique_inverse_y) # Calculate information for cascaded layer  by PWD based upper bound method\n",
        "        finalIX_CAS,finalIY_CAS=calculate_mutual_information(finallayeroutput_cas[0:len(self.DATA)],self.PXs, self.PYs,self.unique_inverse_x,self.unique_inverse_y) # Calculate information for the classifier following the cascaded layer  by PWD based upper bound method\n",
        "        print('KDEI(X;T) is:',IX_CAS,'KDEI(Y;T) is:',IY_CAS)\n",
        "        print('KDEI(X;T) is:',finalIX_CAS,'KDEI(Y;T) is:',finalIY_CAS)\n",
        "        self.KDElocal_IXT_cas_layer.append(IX_CAS)\n",
        "        self.KDElocal_ITY_cas_layer.append(IY_CAS)\n",
        "        self.KDEfinallocal_IXT_cas_layer.append(finalIX_CAS)\n",
        "        self.KDEfinallocal_ITY_cas_layer.append(finalIY_CAS)\n",
        "        \n",
        "#        ================================================================================\n",
        "        IX_CAS1,IY_CAS1=calculate_layer_mutual_information(layeroutput_cas[0:len(self.DATA)],self.PXs, self.PYs,self.unique_inverse_x,self.unique_inverse_y,self.bins) # Calculate information for cascaded layer by Binning method\n",
        "        finalIX_CAS1,finalIY_CAS1=calculate_layer_mutual_information(finallayeroutput_cas[0:len(self.DATA)],self.PXs, self.PYs,self.unique_inverse_x,self.unique_inverse_y,self.bins) # Calculate information for the classifier following the cascaded layer by Binning method\n",
        "        self.local_IXT_cas_layer.append(IX_CAS1)\n",
        "        self.local_ITY_cas_layer.append(IY_CAS1)\n",
        "        self.finallocal_IXT_cas_layer.append(finalIX_CAS1)\n",
        "        self.finallocal_ITY_cas_layer.append(finalIY_CAS1)\n",
        "#        ==============================================================\n",
        "\n",
        "#        self.out_log.append(layeroutput_cas)\n",
        "\"The functions needed for creating network\"\n",
        "def connectOutputBlock(modelToConnectOut,outputsize,activation):\n",
        "    \"The final prediction block\"\n",
        "    modelToConnectOut.add(Dense(outputsize,activation=activation,name='Prediction_Layer'))\n",
        "    return modelToConnectOut\n",
        "def initialblock(structure_nodes,model,X_train,activation,layernum):\n",
        "    \"The initialization setting of the network\"\n",
        "    model.add(Dense(structure_nodes,activation=activation,input_dim=X_train.shape[1],kernel_regularizer=keras.regularizers.l1_l2(l1=0.01, l2=0.01),kernel_initializer=keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None),name='Layer'+str(layernum)))\n",
        "    return model\n",
        "\n",
        "def trainModel(modelToTrain,data,currentEpochs,batch_size,optimizer,accuarcy_name,filepath,layernum,DATA,Label):\n",
        "    modelToTrain.compile(loss='mean_squared_error',optimizer=optimizer,metrics=[accuarcy_name])\n",
        "    bins = np.linspace(-1, 1, 25)\n",
        "    bins = bins.astype(np.float32)\n",
        "    prediction_logs=OutputObserver(np.concatenate((data[0],data[2])),layernum,DATA,np.concatenate((data[1],data[3])),bins)\n",
        "    trainingResults = modelToTrain.fit(data[0],data[1], batch_size=batch_size, epochs=currentEpochs, validation_data=(data[2],data[3]),verbose=1,callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto'),keras.callbacks.ModelCheckpoint(filepath+\"/weights-improvement-{epoch:02d}.tf\", monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1),prediction_logs])\n",
        "    hist = trainingResults.history#keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=nb_epoch/10, verbose=1, mode='auto'),keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, mode='auto'),\n",
        "    return hist,prediction_logs.out_log,prediction_logs.local_IXT_cas_layer,prediction_logs.local_ITY_cas_layer,prediction_logs.finallocal_IXT_cas_layer,prediction_logs.finallocal_ITY_cas_layer,prediction_logs\n",
        "#==================================================================The defination of the parameters we need==============================================================================\n",
        "def parameters(number_of_data,num_of_train):\n",
        "      \"the structure can alter, e.g:list(map(int,np.linspace(2,100,20))) or [100,50,30,20,15,10]\"\n",
        "      input_dimension=12\n",
        "      percent_of_train=np.array([70.00])\n",
        "      structure=[15,10,7,3]# The structure  of the network (The number of nodes in each layer)\n",
        "      outputsize=1          # The dimension of the prediction layer\n",
        "      num_of_experment=str(structure)+'RULar_INFORMATION'+str(percent_of_train)+'_experment_'+str(number_of_data)+'_'+str(num_of_train)  #for change the saving name of results\n",
        "      name_num_of_experment=str(structure)+'_experment_'+str(number_of_data)\n",
        "      outputsize=1 # The size of the output\n",
        "      lr = 0.001  #learning rate\n",
        "      optimizer=Adam(lr=lr) # The name of the optimizer\n",
        "      batch_size=256# The siz of the batch_size\n",
        "      nb_epoch=100 # The number of training epo\n",
        "      activation='tanh'#The activation function\n",
        "      namecase=num_of_experment+'layer_output_case'\n",
        "      accuarcy_name='acc'\n",
        "      use_generated_structure=False\n",
        "      return lr,optimizer,batch_size,nb_epoch,activation,namecase,structure,use_generated_structure,num_of_experment,outputsize,input_dimension,percent_of_train,accuarcy_name,name_num_of_experment"
      ],
      "metadata": {
        "id": "fkD9sdgwhAy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for number_of_data in  ['MNIST']:#['Epileptic']:#,'Epileptic','gisette','LSVT','madelon','arcene','dexter'\n",
        "    for num_of_train in range(1):\n",
        "        lr,optimizer,batch_size,nb_epoch,activation,namecase,structure,use_generated_structure,num_of_experment,outputsize,input_dimension,percent_of_train,accuarcy_name,name_num_of_experment=parameters(number_of_data,num_of_train)\n",
        "        if number_of_data=='MNIST':\n",
        "          (X_train1, Y_train1),(X_test1, Y_test1)=mnist.load_data()\n",
        "          X_train1 = np.array(X_train1.reshape(-1,X_train1.shape[1]*X_train1.shape[2]))\n",
        "          X_test1= np.array(X_test1.reshape(-1,X_test1.shape[1]*X_test1.shape[2]))\n",
        "          Y_train1=Y_train1.reshape(-1,1)\n",
        "          Y_test1=Y_test1.reshape(-1,1)\n",
        "          index_train1=np.where(Y_train1==5)\n",
        "          index_train2=np.where(Y_train1==6)\n",
        "          index_test1=np.where(Y_test1==5) \n",
        "          index_test2=np.where(Y_test1==6)\n",
        "          Y_train1[index_train1]=0\n",
        "          Y_train1[index_train2]=1\n",
        "          Y_test1[index_test1]=0\n",
        "          Y_test1[index_test2]=1\n",
        "          # print('the task is binary classification:',X_train1[index_train1[0]].shape)\n",
        "          X_train=np.concatenate((X_train1[index_train1[0]],X_train1[index_train2[0]]),axis=0)\n",
        "          Y_train=np.concatenate((Y_train1[index_train1[0]],Y_train1[index_train2[0]]))\n",
        "          X_test=np.concatenate((X_test1[index_test1[0]],X_test1[index_test2[0]]),axis=0)\n",
        "          Y_test=np.concatenate((Y_test1[index_test1[0]],Y_test1[index_test2[0]]))\n",
        "          # print('the task is binary classification:',.shape)\n",
        "          # print(Y_test.shape)\n",
        "          Data=np.concatenate((X_train,X_test))\n",
        "          DATA=Data\n",
        "          Label=np.concatenate((Y_train,Y_test))\n",
        "          Label=Label.reshape(-1,1)\n",
        "          print('the data set shape',Data.shape,len(index_train1[0])+len(index_test1[0]))\n",
        "        else:\n",
        "          Data_set=load_generated_data('dataset/'+number_of_data+'.mat')\n",
        "          Data,Label=Data_set['F'],Data_set['y']\n",
        "          DATA=Data\n",
        "          Data_set_shuffled=data_label_shuffle(Data_set, percent_of_train,shuffle_data=True)\n",
        "          X_train, Y_train,X_test, Y_test=Data_set_shuffled.train.data,Data_set_shuffled.train.labels,Data_set_shuffled.test.data,Data_set_shuffled.test.labels\n",
        "          #=============================================================================================================\n",
        "        #                                        Cascade Training process\n",
        "        #=============================================================================================================\n",
        "\n",
        "#        ---------------------------------------------------\n",
        "        print('TRAINING Cascade learning')\n",
        "        history = dict()\n",
        "        output_history=dict()\n",
        "        new_data_set=[X_train, Y_train,X_test, Y_test]\n",
        "        # ==================================================================================\n",
        "        sleep(0.1)\n",
        "        #====================================================================training process==========================================================\n",
        "        local_IXT_cas, local_ITY_cas,finallocal_IXT_cas, finallocal_ITY_cas=[],[],[],[]\n",
        "        KDElocal_IXT_cas, KDElocal_ITY_cas,KDEfinallocal_IXT_cas, KDEfinallocal_ITY_cas=[],[],[],[]\n",
        "        Kdislocal_IXT_cas, Kdislocal_ITY_cas,Kdisfinallocal_IXT_cas, Kdisfinallocal_ITY_cas=[],[],[],[]\n",
        "        for layernum in tqdm.tqdm(range(len(structure))):\n",
        "              if layernum==0:\n",
        "                  model = Sequential()\n",
        "                  model=initialblock(structure[layernum],model,X_train,activation,layernum)\n",
        "                  model = connectOutputBlock(model,outputsize,activation)\n",
        "                  model.summary()\n",
        "                  filepath='check_point_model_layer_save/model_'+num_of_experment+'/layer_'+str(layernum) # The path of saving model\n",
        "                  if not os.path.exists(filepath):\n",
        "                        os.makedirs(filepath)\n",
        "                  history['iter'+str(layernum)] = []\n",
        "                  output_history['iter'+str(layernum)] = []\n",
        "                  hist,prediction_logs_out,local_IXT_cas_layer, local_ITY_cas_layer,finallocal_IXT_cas_layer, finallocal_ITY_cas_layer,prediction_logs=trainModel(model,new_data_set,nb_epoch,batch_size,optimizer,accuarcy_name,filepath,layernum,DATA,Label)\n",
        "                  history['iter'+str(layernum)].append(hist)\n",
        "                  output_history['iter'+str(layernum)].append(prediction_logs_out)\n",
        "                  X_train1,X_test1=X_train,X_test\n",
        "                  model_layer_path='model_layer_save/model_'+num_of_experment # The path of saving model\n",
        "                  if not os.path.exists(model_layer_path):\n",
        "                      os.makedirs(model_layer_path)\n",
        "                  model.save(model_layer_path+'/'+str(layernum)+'model.tf')\n",
        "                  print('layer_'+str(layernum)+'_model is generated')\n",
        "                  local_IXT_cas.append(local_IXT_cas_layer)\n",
        "                  local_ITY_cas.append(local_ITY_cas_layer)\n",
        "                  finallocal_IXT_cas.append(finallocal_IXT_cas_layer)\n",
        "                  finallocal_ITY_cas.append(finallocal_ITY_cas_layer)\n",
        "                  # ========================================\n",
        "                  KDElocal_IXT_cas.append(prediction_logs.KDElocal_IXT_cas_layer)\n",
        "                  KDElocal_ITY_cas.append(prediction_logs.KDElocal_ITY_cas_layer)\n",
        "                  KDEfinallocal_IXT_cas.append(prediction_logs.KDEfinallocal_IXT_cas_layer)\n",
        "                  KDEfinallocal_ITY_cas.append(prediction_logs.KDEfinallocal_ITY_cas_layer)\n",
        "#                  =============================================\n",
        "                  Kdislocal_IXT_cas.append(prediction_logs.kdislocal_IXT_cas_layer)\n",
        "                  Kdislocal_ITY_cas.append(prediction_logs.kdislocal_ITY_cas_layer)\n",
        "                  Kdisfinallocal_IXT_cas.append(prediction_logs.kdisfinallocal_IXT_cas_layer)\n",
        "                  Kdisfinallocal_ITY_cas.append(prediction_logs.kdisfinallocal_ITY_cas_layer)\n",
        "              else:\n",
        "                  intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Layer'+str(layernum-1)).output)\n",
        "                  prediction_class_train_updata=intermediate_layer_model.predict(X_train1)\n",
        "                  prediction_class_test_updata=intermediate_layer_model.predict(X_test1)\n",
        "                  new_data_set=[prediction_class_train_updata, Y_train,prediction_class_test_updata, Y_test]\n",
        "                  del model,intermediate_layer_model\n",
        "                  model= Sequential()\n",
        "                  model=initialblock(structure[layernum],model,prediction_class_train_updata,activation,layernum)\n",
        "                  model = connectOutputBlock(model,outputsize,activation)\n",
        "                  model.summary()\n",
        "                  filepath='check_point_model_layer_save/model_'+num_of_experment+'/layer_'+str(layernum) # The path of saving model\n",
        "                  if not os.path.exists(filepath):\n",
        "                        os.makedirs(filepath)\n",
        "                  history['iter'+str(layernum)] = []\n",
        "                  output_history['iter'+str(layernum)] = []\n",
        "                  hist,prediction_logs_out,local_IXT_cas_layer, local_ITY_cas_layer,finallocal_IXT_cas_layer,finallocal_ITY_cas_layer,prediction_logs=trainModel(model,new_data_set,nb_epoch*(layernum+1),batch_size,optimizer,accuarcy_name,filepath,layernum,DATA,Label)\n",
        "                  history['iter'+str(layernum)].append(hist)\n",
        "                  output_history['iter'+str(layernum)].append(prediction_logs_out)\n",
        "                  X_train1,X_test1=new_data_set[0],new_data_set[2]\n",
        "                  model_layer_path='model_layer_save/model_'+num_of_experment # The path of saving model\n",
        "                  if not os.path.exists(model_layer_path):\n",
        "                    os.makedirs(model_layer_path)\n",
        "                  model.save(model_layer_path+'/'+str(layernum)+'model.tf')\n",
        "                  print('layer_'+str(layernum)+'_model is generated')\n",
        "#                  =========================================================\n",
        "                  local_IXT_cas.append(local_IXT_cas_layer)\n",
        "                  local_ITY_cas.append(local_ITY_cas_layer)\n",
        "                  finallocal_IXT_cas.append(finallocal_IXT_cas_layer)\n",
        "                  finallocal_ITY_cas.append(finallocal_ITY_cas_layer)\n",
        "#                  ===============================================================\n",
        "                  KDElocal_IXT_cas.append(prediction_logs.KDElocal_IXT_cas_layer)\n",
        "                  KDElocal_ITY_cas.append(prediction_logs.KDElocal_ITY_cas_layer)\n",
        "                  KDEfinallocal_IXT_cas.append(prediction_logs.KDEfinallocal_IXT_cas_layer)\n",
        "                  KDEfinallocal_ITY_cas.append(prediction_logs.KDEfinallocal_ITY_cas_layer)\n",
        "#                  =============================================\n",
        "                  Kdislocal_IXT_cas.append(prediction_logs.kdislocal_IXT_cas_layer)\n",
        "                  Kdislocal_ITY_cas.append(prediction_logs.kdislocal_ITY_cas_layer)\n",
        "                  Kdisfinallocal_IXT_cas.append(prediction_logs.kdisfinallocal_IXT_cas_layer)\n",
        "                  Kdisfinallocal_ITY_cas.append(prediction_logs.kdisfinallocal_ITY_cas_layer)\n",
        "#                  ================================================\n",
        "        information_save_path='cascade_information_save/'+num_of_experment\n",
        "        if not os.path.exists(information_save_path):\n",
        "            os.makedirs(information_save_path)\n",
        "        store_file(information_save_path+'/information_x.pkl',local_IXT_cas)\n",
        "        store_file(information_save_path+'/information_y.pkl',local_ITY_cas)\n",
        "        store_file(information_save_path+'/final_information_x.pkl',finallocal_IXT_cas)\n",
        "        store_file(information_save_path+'/final_information_y.pkl',finallocal_ITY_cas)\n",
        "#        =================================================================\n",
        "        # with sess.as_default():\n",
        "        #      KDElocal_IXT_cas=KDElocal_IXT_cas.eval()\n",
        "        #      KDElocal_ITY_cas=KDElocal_ITY_cas.eval()\n",
        "        #      KDEfinallocal_IXT_cas=KDEfinallocal_IXT_cas.eval()\n",
        "        #      KDEfinallocal_ITY_cas=KDEfinallocal_ITY_cas.eval()\n",
        "        # print('KDEI(X;T) is:',IX_CAS,'KDEI(Y;T) is:',IY_CAS)\n",
        "        # print('KDEI(X;T) is:',finalIX_CAS,'KDEI(Y;T) is:',finalIY_CAS)\n",
        "        store_file(information_save_path+'/KDEinformation_x.pkl',KDElocal_IXT_cas)\n",
        "        store_file(information_save_path+'/KDEinformation_y.pkl',KDElocal_ITY_cas)\n",
        "        store_file(information_save_path+'/KDEfinal_information_x.pkl',KDEfinallocal_IXT_cas)\n",
        "        store_file(information_save_path+'/KDEfinal_information_y.pkl',KDEfinallocal_ITY_cas)\n",
        "#        ======================================================================\n",
        "        store_file(information_save_path+'/Kdisinformation_x.pkl',Kdislocal_IXT_cas)\n",
        "        store_file(information_save_path+'/Kdisinformation_y.pkl',Kdislocal_ITY_cas)\n",
        "        store_file(information_save_path+'/Kdisfinal_information_x.pkl',Kdisfinallocal_IXT_cas)\n",
        "        store_file(information_save_path+'/Kdisfinal_information_y.pkl',Kdisfinallocal_ITY_cas)\n",
        "#        =========================================================================\n",
        "\n",
        "        #========================================================plot training process=======================================================\n",
        "        accuarcy_save_path='history_accuarcy/acc_'+num_of_experment\n",
        "        if not os.path.exists(accuarcy_save_path):\n",
        "            os.makedirs(accuarcy_save_path)\n",
        "        store_file(accuarcy_save_path+'/'+'acc.pkl',history)\n",
        "        store_file(accuarcy_save_path+'/'+'output_history.pkl',output_history)\n",
        "        print('history is saved')\n",
        "       #=====================================================================================================================================\n",
        "        HISTORY=load_DATA(accuarcy_save_path+'/'+'acc.pkl')\n",
        "        hISTORY=dict()\n",
        "        for layernum in range(len(structure)):\n",
        "           hISTORY['iter_TRAIN'+str(layernum)]=[]\n",
        "           hISTORY['iter_TEST'+str(layernum)]=[]\n",
        "           for i in range(len(HISTORY['iter'+str(layernum)])):\n",
        "                 hISTORY['iter_TRAIN'+str(layernum)].append(HISTORY['iter'+str(layernum)][i][accuarcy_name])\n",
        "                 hISTORY['iter_TEST'+str(layernum)].append(HISTORY['iter'+str(layernum)][i]['val_'+accuarcy_name])\n",
        "       #--------------------------------------------------------------------------------------------------------------------------------------\n",
        "        accuarcy_image_save_path='image_history_accuarcy/acc_'+num_of_experment\n",
        "        if not os.path.exists(accuarcy_image_save_path):\n",
        "           os.makedirs(accuarcy_image_save_path)\n",
        "        fig_all=plt.figure(figsize=(15,8))#figsize=(10, 10)\n",
        "        axis_font=28\n",
        "        axes=fig_all.add_subplot(1,2,1)\n",
        "        for layernum in range(len(structure)):\n",
        "           axes.plot(hISTORY['iter_TRAIN'+str(layernum)][0],label='CAS_train_acc_layer'+str(layernum))\n",
        "           axes.text(0.5*len(hISTORY['iter_TRAIN'+str(0)][0]),0.5+0.1*layernum,'Layer_'+str(layernum)+'_accuarcy is:'+str(hISTORY['iter_TRAIN'+str(layernum)][0][-1]))\n",
        "        plt.legend()\n",
        "        # plt.grid(True)\n",
        "        axes.set_xlim(-0.1)\n",
        "        axes.set_ylim(-0.1)\n",
        "        axes.set_yticks(np.linspace(0,1,11))\n",
        "        plt.title('ACC_TRAIN',fontsize=axis_font+2)\n",
        "        plt.xlabel('Epoch',fontsize=axis_font)\n",
        "        plt.ylabel('ACC',fontsize=axis_font)\n",
        "        axes=fig_all.add_subplot(1,2,2)\n",
        "        for layernum in range(len(structure)):\n",
        "           axes.plot(hISTORY['iter_TEST'+str(layernum)][0],label='CAS_test_acc_layer'+str(layernum))\n",
        "           axes.text(0.5*len(hISTORY['iter_TRAIN'+str(0)][0]),0.5+0.1*layernum,'Layer_'+str(layernum)+'_accuarcy is:'+str(hISTORY['iter_TEST'+str(layernum)][0][-1]))\n",
        "        plt.legend()\n",
        "        # plt.grid(True)\n",
        "        axes.set_xlim(-0.1)\n",
        "        axes.set_ylim(-0.1)\n",
        "        axes.set_yticks(np.linspace(0,1,11))\n",
        "        plt.title('ACC_TEST',fontsize=axis_font+2)\n",
        "        plt.xlabel('Epoch',fontsize=axis_font)\n",
        "        plt.ylabel('ACC',fontsize=axis_font)\n",
        "        plt.suptitle('all in one'+num_of_experment)\n",
        "       #plt.show()\n",
        "        fig_all.savefig(accuarcy_image_save_path+'/accuarcy.png')\n",
        "        fig_all.savefig('image_history_accuarcy/accuarcy'+num_of_experment+'.png')\n",
        "       # print(output_history)\n"
      ],
      "metadata": {
        "id": "Tngdku4phJcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YgxOA4qot2F"
      },
      "source": [
        "#Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY9aWMaVzq-M"
      },
      "outputs": [],
      "source": [
        "!cp /content/model_layer_save -r drive/MyDrive/Cascade/tanh/\n",
        "!cp /content/check_point_model_layer_save -r drive/MyDrive/Cascade/tanh/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/Cascade/tanh/ -r /content/model_layer_save\n",
        "!cp  drive/MyDrive/Cascade/tanh/ -r /content/check_point_model_layer_save "
      ],
      "metadata": {
        "id": "4ZgwjNsWk33S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yNrALytolGb"
      },
      "outputs": [],
      "source": [
        "!cp drive/MyDrive/Cascade/plot.py /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlpKhRafoX59"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import pickle as cPickle\n",
        "import numpy as np\n",
        "import plot as plo\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from training_parameters import parameters # Import parametrs used for training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5ly6FnPrmvj"
      },
      "outputs": [],
      "source": [
        "def load_DATA(name):\n",
        "    \"Load saved data\"\n",
        "    with open(name, 'rb') as f:\n",
        "        data=cPickle.load(f)\n",
        "        return data \n",
        "\n",
        "def parameters(number_of_data,num_of_train):\n",
        "      \"the structure can alter, e.g:list(map(int,np.linspace(2,100,20))) or [100,50,30,20,15,10]\"\n",
        "#      input_dimension=12\n",
        "      percent_of_train=np.array([70.00])\n",
        "      structure=[20,10,7,3]#[20,10,7,3]# The structure  of the network (The number of nodes in each layer)\n",
        "      outputsize=1          # The dimension of the prediction layer\n",
        "      num_of_experment='check_weight_no_stop'+str(structure)+'RULar_INFORMATION'+str(percent_of_train)+'_'+'experment_'+str(number_of_data)+'_'+str(num_of_train)#'test_INFORMATION_version_'+str(percent_of_train)+'_'+str(structure)+'_full_size_experment_'+str(number_of_data)+'_'+str(num_of_train)#str(structure)+'RULar_INFORMATION'+str(percent_of_train)+'_'+'experment_'+str(number_of_data)+'_'+str(num_of_train)#for change the saving name of results#'structure_test_cascade_learning'+str(percent_of_train)+'_'+str(structure)+'_'+'experment_'+str(number_of_data)+'_'+str(num_of_train)#'test_KDE_save_cascade_learning'+str(percent_of_train)+'_'+str(structure)+'_experment_'+str(number_of_data)+'_'+str(num_of_train)   #str(structure)+'RULar_INFORMATION'+str(percent_of_train)+'_'+'experment_'+str(number_of_data)+'_'+str(num_of_train)#for change the saving name of results\n",
        "      name_num_of_experment=str(structure)+'_experment_'+str(number_of_data)\n",
        "      outputsize=1 # The size of the output\n",
        "      nb_epoch=3000 # The number of training epoch\n",
        "      num_of_bins=25\n",
        "      bins = np.linspace(-1, 1, num_of_bins)\n",
        "      bins = bins.astype(np.float32)\n",
        "      accuarcy_name='acc'\n",
        "      return nb_epoch,structure,num_of_experment,outputsize,percent_of_train,name_num_of_experment,bins,accuarcy_name\n",
        "\n",
        "xticks = np.linspace(0,10,11)\n",
        "yticks = np.linspace(0,0.5,6)\n",
        "x_lim=-0\n",
        "y_lim=-0\n",
        "axis_font=25\n",
        "fig_size = (16,8)\n",
        "font_size = 25    \n",
        "#======================================================================\n",
        "def plot_layer_sepreate(local_IXT_CAS,local_ITY_CAS,finallocal_IXT_CAS,finallocal_ITY_CAS,structure,num_of_experment):\n",
        "        fig1=plt.figure(figsize=(15, 8))\n",
        "        local_IXT_CAS_array,local_ITY_CAS_array=np.array(local_IXT_CAS),np.array(local_ITY_CAS)\n",
        "        finallocal_IXT_CAS_array,finallocal_ITY_CAS_array=np.array(finallocal_IXT_CAS),np.array(finallocal_ITY_CAS)\n",
        "        for layernum in range(len(structure)):\n",
        "            ax=fig1.add_subplot(2,np.ceil(len(structure)/2),layernum+1)\n",
        "            ax.scatter(local_IXT_CAS_array[layernum],local_ITY_CAS_array[layernum],label='CAS_layer'+str(layernum))\n",
        "            print('the epoch is:'+str(len(local_IXT_CAS_array[layernum])))\n",
        "            ax.scatter(local_IXT_CAS_array[layernum][-1],local_ITY_CAS_array[layernum][-1],label='CAS_layer'+str(layernum)+'final_epoch',facecolors='none', edgecolors='b')\n",
        "            ax.scatter(finallocal_IXT_CAS_array[layernum],finallocal_ITY_CAS_array[layernum],label='finalCAS_layer'+str(layernum)) \n",
        "            ax.scatter(finallocal_IXT_CAS_array[layernum][-1],finallocal_ITY_CAS_array[layernum][-1],label='finalCAS_layer'+str(layernum)+'final_epoch',facecolors='none', edgecolors='g') \n",
        "            fig1.suptitle('information_comparison_'+num_of_experment,fontsize=axis_font + 2)\n",
        "            ax.set_xticks(xticks)\n",
        "            ax.set_yticks(yticks)\n",
        "            ax.set_xlim(x_lim)\n",
        "            ax.set_ylim(y_lim)\n",
        "            plt.xlabel('$I(X;T)$',fontsize=axis_font)\n",
        "            plt.ylabel('$I(Y;T)$',fontsize=axis_font)\n",
        "            plt.legend()\n",
        "        # plt.show()\n",
        "        information_figure_path='Cascade_information_result_saving/'+num_of_experment\n",
        "        finalinformation_figure_path='Cascade_final_information_result_saving/'+num_of_experment\n",
        "        if not os.path.exists(information_figure_path):\n",
        "            os.makedirs(information_figure_path)\n",
        "        if not os.path.exists(finalinformation_figure_path):\n",
        "            os.makedirs(finalinformation_figure_path)\n",
        "        fig1.savefig(information_figure_path+'/'+'hidden_final_compare.png')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for number_of_data in ['MNIST']:#Epileptic,gisette\n",
        "    for num_of_train in range(1):\n",
        "        nb_epoch,structure,num_of_experment,outputsize,percent_of_train,name_num_of_experment,bins,accuarcy_name=parameters(number_of_data,num_of_train)\n",
        "        \n",
        "        ###===========================plot training perfprmance========================\n",
        "        accuarcy_save_path='history_accuarcy/acc_'+num_of_experment\n",
        "        HISTORY=load_DATA(accuarcy_save_path+'/acc.pkl')\n",
        "        max_epoch=np.array([len(HISTORY['iter'+str(layernum)][0][accuarcy_name]) for layernum in range(len(structure))]).max()\n",
        "        print('The maximium epoch is:',str(max_epoch))\n",
        "\n",
        "        epochsInds=np.linspace(1,max_epoch,max_epoch)\n",
        "        hISTORY=dict()\n",
        "        for layernum in range(len(structure)):\n",
        "            hISTORY['iter_TRAIN'+str(layernum)]=[]\n",
        "            hISTORY['iter_TEST'+str(layernum)]=[]\n",
        "            for i in range(len(HISTORY['iter'+str(layernum)])):\n",
        "                  hISTORY['iter_TRAIN'+str(layernum)].append(HISTORY['iter'+str(layernum)][i][accuarcy_name]) \n",
        "                  hISTORY['iter_TEST'+str(layernum)].append(HISTORY['iter'+str(layernum)][i]['val_'+accuarcy_name])\n",
        "        #--------------------------------------------------------------------------------------------------------------------------------------\n",
        "        accuarcy_image_save_path='image_history_accuarcy/acc_'+num_of_experment\n",
        "        if not os.path.exists(accuarcy_image_save_path):\n",
        "            os.makedirs(accuarcy_image_save_path)\n",
        "        fig_all=plt.figure(figsize=(16,10))\n",
        "        axis_font=28\n",
        "        axes=fig_all.add_subplot(1,2,1)\n",
        "        for layernum in range(len(structure)):\n",
        "            axes.plot(hISTORY['iter_TRAIN'+str(layernum)][0],label='Layer '+str(layernum)+', Average:'+str(str(round(np.mean(hISTORY['iter_TRAIN'+str(layernum)][0][-100:-1]),3))))\n",
        "            # axes.text(200,0.9+0.02*layernum,'Layer '+str(layernum)+':'+str(round(np.mean(hISTORY['iter_TRAIN'+str(layernum)][0][-100:-1]),3)),fontsize=axis_font-6)#0.5*len(hISTORY['iter_TRAIN'+str(layernum)][0])\n",
        "        plt.legend(fontsize=axis_font-10,loc=1,ncol=1,bbox_to_anchor=(0.98,0.75))\n",
        "        # plt.grid(True)\n",
        "        axes.set_xlim(-0.1)\n",
        "        axes.set_ylim(0.4)\n",
        "        axes.set_yticks(np.linspace(0.4,1.1,7)) \n",
        "        # plt.title('ACC_TRAIN',fontsize=axis_font+2)\n",
        "        plt.xlabel('Epoch\\n (a) Training accuarcy',fontsize=axis_font)\n",
        "        plt.ylabel('Accuarcy',fontsize=axis_font)\n",
        "        plt.xticks(rotation=45,fontsize=22)\n",
        "        plt.yticks(fontsize=22)\n",
        "        # ===================================================================\n",
        "        rect = [0.36,0.12,0.6,0.4]\n",
        "        box = axes.get_position()\n",
        "        width = box.width\n",
        "        height = box.height\n",
        "        inax_position  = axes.transAxes.transform(rect[0:2])\n",
        "        transFigure = fig_all.transFigure.inverted()\n",
        "        infig_position = transFigure.transform(inax_position)    \n",
        "        xl = infig_position[0]\n",
        "        yl = infig_position[1]\n",
        "        width *= rect[2]\n",
        "        height *= rect[3]  # <= Typo was here\n",
        "        ax2= fig_all.add_axes([xl,yl,width,height])\n",
        "        for layernum in range(len(structure)):\n",
        "            ax2.plot(hISTORY['iter_TRAIN'+str(layernum)][0][0:50],label='Layer '+str(layernum))\n",
        "        plt.xlabel('Epoch',fontsize=axis_font-2)\n",
        "        plt.ylabel('Accuarcy',fontsize=axis_font-2)\n",
        "        plt.xticks(fontsize=22)\n",
        "        plt.yticks(fontsize=22)\n",
        "        plt.legend(fontsize=axis_font-10,loc='3')\n",
        "        # ======================================================================\n",
        "        axes1=fig_all.add_subplot(1,2,2) \n",
        "        for layernum in range(len(structure)):\n",
        "            axes1.plot(hISTORY['iter_TEST'+str(layernum)][0],label='Layer '+str(layernum)+', Average:'+str(str(round(np.mean(hISTORY['iter_TEST'+str(layernum)][0][-100:-1]),3))))\n",
        "            # axes1.text(200,0.9+0.02*layernum,'Layer '+str(layernum)+':'+str(round(np.mean(hISTORY['iter_TEST'+str(layernum)][0][-100:-1]),3)),fontsize=axis_font-6)\n",
        "        plt.legend(fontsize=axis_font-10,loc=1,ncol=1,bbox_to_anchor=(0.98,0.75))\n",
        "        # plt.grid(True)\n",
        "        axes1.set_xlim(-0.1)\n",
        "        axes1.set_ylim(0.4)\n",
        "        axes1.set_yticks(np.linspace(0.4,1.1,7))\n",
        "        # plt.title('ACC_TEST',fontsize=axis_font+2)\n",
        "        plt.xlabel('Epoch\\n (b) Testing accuarcy',fontsize=axis_font)\n",
        "        # plt.ylabel('Accuarcy',fontsize=axis_font)\n",
        "        plt.xticks(rotation=45,fontsize=22)\n",
        "        plt.yticks(fontsize=22)\n",
        "        # ===========================================================================\n",
        "        rect = [0.36,0.12,0.6,0.4]\n",
        "        box = axes1.get_position()\n",
        "        width = box.width\n",
        "        height = box.height\n",
        "        inax_position  = axes1.transAxes.transform(rect[0:2])\n",
        "        transFigure = fig_all.transFigure.inverted()\n",
        "        infig_position = transFigure.transform(inax_position)    \n",
        "        xl = infig_position[0]\n",
        "        yl = infig_position[1]\n",
        "        width *= rect[2]\n",
        "        height *= rect[3]  # <= Typo was here\n",
        "        ax1= fig_all.add_axes([xl,yl,width,height])\n",
        "        for layernum in range(len(structure)):\n",
        "            ax1.plot(hISTORY['iter_TEST'+str(layernum)][0][0:50],label='Layer '+str(layernum))\n",
        "        plt.xlabel('Epoch',fontsize=axis_font-2)\n",
        "        plt.ylabel('Accuarcy',fontsize=axis_font-2)\n",
        "        plt.xticks(fontsize=22)\n",
        "        plt.yticks(fontsize=22)\n",
        "        plt.legend(fontsize=axis_font-10,loc='3')\n",
        "        plt.show()\n",
        "        # ==========================================================================\n",
        "        # plt.suptitle('all in one'+num_of_experment)\n",
        "        #plt.show()\n",
        "        fig_all.savefig(accuarcy_image_save_path+'/accuarcy.png',bbox_inches='tight')\n",
        "        fig_all.savefig('image_history_accuarcy/accuarcy'+num_of_experment+'.png',bbox_inches='tight')\n",
        "\n",
        "        local_IXT_CAS=load_DATA(information_save_path+'/KDEinformation_x.pkl')\n",
        "        local_ITY_CAS=load_DATA(information_save_path+'/KDEinformation_y.pkl')\n",
        "        finallocal_IXT_CAS=load_DATA(information_save_path+'/KDEfinal_information_x.pkl')\n",
        "        finallocal_ITY_CAS=load_DATA(information_save_path+'/KDEfinal_information_y.pkl')\n",
        "        #==============================plot final============================================\n",
        "        information_figure_path='Cascade_information_result_saving/KDE'+num_of_experment\n",
        "        finalinformation_figure_path='Cascade_final_information_result_saving/KDE'+num_of_experment\n",
        "        if not os.path.exists(information_figure_path):\n",
        "            os.makedirs(information_figure_path) \n",
        "        if not os.path.exists(finalinformation_figure_path):\n",
        "            os.makedirs(finalinformation_figure_path)\n",
        "        plot_layer_sepreate(local_IXT_CAS,local_ITY_CAS,finallocal_IXT_CAS,finallocal_ITY_CAS,structure,'KDE'+num_of_experment)\n",
        "        local_IXT_CAS_array,local_ITY_CAS_array=np.array(local_IXT_CAS),np.array(local_ITY_CAS)\n",
        "        finallocal_IXT_CAS_array,finallocal_ITY_CAS_array=np.array(finallocal_IXT_CAS),np.array(finallocal_ITY_CAS)\n",
        "        plo.plotfigure(local_IXT_CAS_array,local_ITY_CAS_array,epochsInds,information_figure_path)\n",
        "        plo.plotfigure(finallocal_IXT_CAS_array,finallocal_ITY_CAS_array,epochsInds,finalinformation_figure_path)\n",
        "        "
      ],
      "metadata": {
        "id": "6GnG4-PQ8KZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sm = plt.cm.ScalarMappable(cmap='gnuplot', norm=plt.Normalize(vmin=0, vmax=100))\n",
        "sm._A = []\n",
        "PLOT_LAYERS    = None \n",
        "num_layers = 5\n",
        "\n",
        "activations = ['tanh', 'relu']\n",
        "\n",
        "if PLOT_LAYERS is None:\n",
        "        PLOT_LAYERS = []\n",
        "        for lndx in range(num_layers):\n",
        "            #if d['data']['activity_tst'][lndx].shape[1] < 200 and lndx != num_layers - 1:\n",
        "            PLOT_LAYERS.append(lndx)\n",
        "fig=plt.figure(figsize=(5,5))\n",
        "\n",
        "\n",
        "    \n",
        "for epoch in range(1, epochsInds.shape[0]):\n",
        "    c = sm.to_rgba(epoch)\n",
        "    xmvals = local_IXT_CAS_array[:,epoch]\n",
        "    ymvals = local_ITY_CAS_array[:, epoch]\n",
        "    plt.plot(xmvals, ymvals, c=c, alpha=0.1, zorder=1)\n",
        "    plt.scatter(xmvals, ymvals, s=20, facecolors=[c for _ in PLOT_LAYERS], edgecolor='none', zorder=2)\n",
        "\n",
        "\n",
        "plt.ylim([0, 1])\n",
        "plt.xlim([0, 12])\n",
        "#     plt.ylim([0, 3.5])\n",
        "#     plt.xlim([0, 14])\n",
        "plt.xlabel('I(X;M)')\n",
        "plt.ylabel('I(Y;M)')\n",
        "plt.title('tanh')\n",
        "    \n",
        "cbaxes = fig.add_axes([1.0, 0.125, 0.03, 0.8]) \n",
        "plt.colorbar(sm, label='Epoch', cax=cbaxes)\n",
        "\n",
        "DO_SAVE = True\n",
        "\n",
        "!mkdir /content/plots\n",
        "if DO_SAVE:\n",
        "    plt.savefig('plots/' + 'Synthetic tanh batch')\n",
        "!cp /content/plots -r drive/MyDrive/Cascade/tanh"
      ],
      "metadata": {
        "id": "rR8Zxe7tGpr1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}